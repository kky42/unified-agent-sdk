# Token Usage

This document specifies how unified-agent-sdk records and interprets token usage for agent turns, and why the numbers can differ between Claude and Codex.

## What the SDK reports

On `run.completed`, adapters may attach `usage`:

- `input_tokens`: tokens sent to the model for that turn
- `output_tokens`: tokens generated by the model for that turn
- `total_tokens`: `input_tokens + output_tokens`
- `cache_read_tokens`: input tokens served from prompt cache (if the provider supports it)
- `cache_write_tokens`: input tokens written to prompt cache (if the provider supports it)
- `context_length`: best-effort estimate of the model context length for the **final model call** that produced the assistant response for that turn (typically `input_tokens + output_tokens` for that call)

Important:

- `run.completed.usage` is the **total usage for the turn**. It can include multiple internal model calls and tool loops inside the provider.
- `usage.context_length` is a best-effort “how big was the final call” signal; it is not a cost metric and may be unavailable.

## Provider behavior differences

### Claude (Claude Code)

- Claude Code usage reported at the end of a turn can be **aggregated across multiple internal model calls**. The unified Claude adapter reports this aggregate as `run.completed.usage`.
- For “context length” consumers, the adapter also derives `usage.context_length` from the most recent streaming `message_delta.usage` snapshot (fallback to aggregate totals when unavailable).

### Codex (Codex CLI)

Codex behaves differently in multi-turn sessions:

- Codex CLI injects repository and environment context (notably **`AGENTS.md` instructions** and **`<environment_context>`**) as **new user messages on every turn**.
- Those injected messages become part of the conversation history, so the effective context grows faster than “just the user prompt + assistant replies”.
- This is why, for Codex, “context length” can grow quickly in continuous sessions even when user prompts are small.

Adapter behavior:

- Codex reports cumulative session totals in `turn.completed.usage`. The unified Codex adapter computes a per-turn delta and reports it as `run.completed.usage`.
- To estimate “context length”, the adapter reads Codex CLI session logs and derives `usage.context_length` from the last `token_count` event observed for the turn (`last_token_usage.input_tokens + last_token_usage.output_tokens`).

#### How to verify (Codex)

Codex stores sessions under `CODEX_HOME/sessions/**/rollout-*.jsonl`. In a multi-turn session, you will typically see one occurrence per turn of:

- `# AGENTS.md instructions for ...`
- `<environment_context>`

Quick sanity checks:

```bash
rg -c "# AGENTS.md instructions" ~/.codex/sessions/**/rollout-*.jsonl
rg -c "<environment_context>" ~/.codex/sessions/**/rollout-*.jsonl
```

## Comparison: real multi-turn run (12 turns)

Run config:

- `session-mode=continuous` (one session, many turns)
- `filler-words=1200` (stable instruction-file filler to encourage caching)
- `reasoningEffort=low`
- Claude: `model=haiku`
- Codex: `model=gpt-5.2`
- Generated: 2026-01-30

Repro command:

```bash
npm run verify:token-usage:real -- \
  --provider=both \
  --session-mode=continuous \
  --turns=12 \
  --filler-words=1200 \
  --claude-model=haiku \
  --codex-model=gpt-5.2 \
  --prompt "You are in a multi-turn test.\nDo not use any tools.\nReply in plain text only.\nReply exactly: OK"
```

Per-turn usage (input/output/total) and cache breakdown:

| provider | model | reasoningEffort | turn | input_tokens | output_tokens | total_tokens | cache_read_tokens | cache_write_tokens |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| claude | haiku | low | 1 | 16494 | 92 | 16586 | 0 | 16484 |
| claude | haiku | low | 2 | 16541 | 59 | 16600 | 13325 | 3206 |
| claude | haiku | low | 3 | 16587 | 85 | 16672 | 15295 | 1282 |
| claude | haiku | low | 4 | 16633 | 83 | 16716 | 15341 | 1282 |
| claude | haiku | low | 5 | 16679 | 61 | 16740 | 15387 | 1282 |
| claude | haiku | low | 6 | 16725 | 96 | 16821 | 15433 | 1282 |
| claude | haiku | low | 7 | 16771 | 81 | 16852 | 15479 | 1282 |
| claude | haiku | low | 8 | 16817 | 68 | 16885 | 15525 | 1282 |
| claude | haiku | low | 9 | 16863 | 81 | 16944 | 15571 | 1282 |
| claude | haiku | low | 10 | 16909 | 56 | 16965 | 15617 | 1282 |
| claude | haiku | low | 11 | 17057 | 54 | 17111 | 15663 | 1384 |
| claude | haiku | low | 12 | 17103 | 64 | 17167 | 15709 | 1384 |
| codex | gpt-5.2 | low | 1 | 13553 | 29 | 13582 | 3840 | 0 |
| codex | gpt-5.2 | low | 2 | 15786 | 5 | 15791 | 13440 | 0 |
| codex | gpt-5.2 | low | 3 | 18019 | 5 | 18024 | 15744 | 0 |
| codex | gpt-5.2 | low | 4 | 20252 | 5 | 20257 | 17920 | 0 |
| codex | gpt-5.2 | low | 5 | 22485 | 5 | 22490 | 20224 | 0 |
| codex | gpt-5.2 | low | 6 | 24718 | 5 | 24723 | 22400 | 0 |
| codex | gpt-5.2 | low | 7 | 26951 | 5 | 26956 | 24576 | 0 |
| codex | gpt-5.2 | low | 8 | 29184 | 5 | 29189 | 26880 | 0 |
| codex | gpt-5.2 | low | 9 | 31417 | 5 | 31422 | 29056 | 0 |
| codex | gpt-5.2 | low | 10 | 33650 | 5 | 33655 | 31360 | 0 |
| codex | gpt-5.2 | low | 11 | 35883 | 5 | 35888 | 33536 | 0 |
| codex | gpt-5.2 | low | 12 | 38116 | 5 | 38121 | 35840 | 0 |

Notes:

- The Codex per-turn prompt was intentionally tiny, but `input_tokens` still grows quickly because Codex injects repo/environment context every turn and it accumulates in the thread history.
